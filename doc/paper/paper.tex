% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{commath}
\usepackage{pslatex}
\usepackage{apacite}


\title{TODO title}
 
\author{{\large \bf Jan Gosmann (jgosmann@uwaterloo.ca)} \\
  %Department of Psychology, 1202 W. Johnson Street \\
  %Madison, WI 53706 USA
  \AND{\large \bf Aaron Voelker (TODO)} \\
  \AND{\large \bf Chris Eliasmith (TODO)}
  %Department of Educational Psychology, 1025 W. Johnson Street \\
  %Madison, WI 53706 USA}
  }


\begin{document}

\maketitle


\begin{abstract}
The abstract should be one paragraph, indented 1/8~inch on both sides,
in 9~point font with single spacing. The heading ``{\bf Abstract}''
should be 10~point, bold, centered, with one line of space below
it. This one-paragraph abstract section is required only for standard
six page proceedings papers. Following the abstract should be a blank
line, followed by the header ``{\bf Keywords:}'' and a list of
descriptive keywords separated by semicolons, all in 9~point font, as
shown below.

\textbf{Keywords:} 
add your choice of indexing terms or keywords; kindly use a
semicolon; between each term
\end{abstract}


\section{Introduction}
Winner-take-all (WTA) mechanisms are often employed in cognitive models. For 
example, TODO O'Reilly, Nengo cleanup, TCM\@. A WTA mechanism receives 
a $d$-dimensional input of utility values for $d$ different choices. The output 
is supposed to be larger than zero for the dimension with highest utility and 
zero for all other inputs.

A large body of literature exists examining how well different WTA mechanisms 
can fit different data from decision experiments (TODO references). Here, 
however, we will investigate the suitability of two different WTA mechanisms in 
the context of large-scale cognitive modelling with spiking neurons on a set of 
benchmarks that are more normative in nature.  The first mechanism is an 
implementation of the classic Usher-McClelland (TODO ref) leaky, competing 
accumulator model.  It has been widely used, for example in versions of the TCM 
model (TODO ref), and TODO some of our models. We will compare this to an 
independent accumulator model with a second thresholding layer with recurrent 
connections to the first layer. In both cases we are especially interested in 
situations with more than two ($d > 2$) choices.

For the implementation of these models we use the Neural Engineering Framework 
(NEF) which allows us to directly implement the prescribed dynamics with spiking 
neurons. The NEF has been used in a wide range of models TODO, including the 
largest functional brain model Spaun (TODO ref). We will give a short 
introduction to the NEF first, then describe the implementation of the two WTA 
mechanisms. In section TODO, we give the results on a number of metrics, 
followed by a discussion in section TODO\@.

\section{Methods}

\subsection{The Neural Engineering Framework}
We employ the Neural Engineering Framework (NEF, TODO cite) which allows us to 
implement mathematical model descriptions in spiking neural networks based on 
three principles: \emph{representation}, \emph{transformation}, and 
\emph{dynamics}.

\subsubsection{Principle 1: Representation}
The representation of a value $x(t)$ with a population of spiking neurons is 
defined by the encoding and decoding. The encoding converting the value $x$ into 
a spike train or instantaneous firing rate $a_i(t)$ for neuron $i$ is given by
\begin{equation}
    a_i(t) = G_i\left[\alpha_i x(t) + J_i^{\mathrm{bias}}\right]
\end{equation}
where $\alpha_i$ is a gain factor, $J_i^{mathrm{bias}}$ a bias current and $G_i$ 
is the neuron linearity. Here, we use spiking leaky integrate-and-fire neurons 
for $G_i$.

Decoding weights $d_i$ are used to decode a value $\hat x(t)$ from such 
a population of neurons with
\begin{equation}
    \hat x(t) = \sum_i d_i \left[a_i(t) * h(t)\right]
\end{equation}
where $h(t) = \tau^{-1}\exp(1/\tau)$ is the synaptic filter. The decoding 
weights are obtained with a least-squares optimization of the error $E^x 
= \abs{x - \hat x}$.

For the transmission of a value from one population to another, the connection 
weights are given by
\begin{equation}
    W_{ij} = \alpha_j d_i \text{.}
\end{equation}

\subsubsection{Principle 2: Transformation}
By finding alternate decoding weights $d^f_i$ with the error given by $E^{f(x)} 
= \abs{f(x) - \hat x}$ arbitrary linear and non-linear functions $f(x)$ can be 
approximated in the connections between neural populations.

\subsubsection{Principle 3: Dynamics}
TODO

\subsection{Leaky, competing accumulator model}
We implement the widely used, leaky, competing accumulator model proposed by 
TODO\@. The dynamics for the state variables $x_i, i < N$, where $N$ is the 
number of choices, are given by
\begin{equation}\label{eqn:usher-mcclelland}
    \begin{split}
    dx_i &= \left(\rho_i - kx_i - \beta \sum_{j \neq i} x_j\right) 
    \frac{dt}{\tau'} \\
    x_i &\rightarrow \max(x_i, 0)
    \end{split}
\end{equation}
where $\rho_i$ is the external input, $k$ is the leak rate, $\beta$ the lateral 
inhibition, and $tau'$ the time constant. Note that one usually wants to set $k 
= \beta$ because TODO\@. Here, we chose $k = \beta = 1$. TODO what does changing 
this control? Why can we fix it at 1?

Equation~\ref{eqn:usher-mcclelland} can easily be implemented with the NEF by 
using one population of neurons for each $x_i$.  We believe this implementation 
to be novel as it it does not treat the $x_i$ as neural firing rates, rather the 
neural firing rates are weighted by the decoding weights to precisely implement 
the stated dynamics. As such the model equations do not impose restrictions on 
the neuron model or requires to lump the neurons into a single population firing 
rate. This allows us to use biologically more realistic neurons while at the 
same time adhering to the dynamics prescribed by the model.

TODO figure with dynamics

\subsection{Independent accumulator model}
The other Winner-take-all mechanism is related to the Usher-McClelland model and 
even more closely to independent accumulator models (TODO refs), but has some 
fine and important distinctions. It consists of two layers of neural 
populations. The first layer are independent accumulators obtained from 
Equation~\ref{eqn:usher-mcclelland} by setting $k = \beta = 0$ resulting in
\begin{equation}
    \begin{split}
    dx^1_i &= \lambda \rho_i \frac{dt}{\tau} \\
    x^1_i &\rightarrow \max(x^1_i, 0) \text{.}
    \end{split}
\end{equation}
The parameter $\lambda$ scales this input and is introduced because it is more 
convenient to manipulate than then the threshold parameter $\vartheta$ 
introduced below.

The second layer consists of independent non-recurrent populations that get 
input from the first layer in a one to one projection. We decode the shifted 
Heaviside function $\Theta(x^2_i - \vartheta)$ from it where $\vartheta = 0.8$ 
is a threshold value controlling how much evidence needs to be accumulated to 
produce an output. Instead of controlling this threshold, it is more convenient 
to scale the input with $\lambda$ in the context of the NEF, though 
mathematically both approaches are equivalent. As the Usher-McClelland does not 
have a threshold parameter, we do not introduce the input scaling $\lambda$ in 
that model.

The Heaviside decoded output of layer 2 projects back to layer 1 to add $x^2_i 
- 2\sum_{j \neq i} x^2_j$ to the input of $x^1_i$. As effect the choice will be 
stabilized while all other state variables will be suppressed once one of the 
accumulators reaches the threshold $\vartheta$.

TODO figure with dynamics

\section{Results}

\section{Discussion}
* ever increasing evidence

\section{Acknowledgments}

Place acknowledgments (including funding information) in a section at
the end of the paper.


\section{References Instructions}

Follow the APA Publication Manual for citation format, both within the
text and in the reference list, with the following exceptions: (a) do
not cite the page numbers of any book, including chapters in edited
volumes; (b) use the same format for unpublished references as for
published ones. Alphabetize references by the surnames of the authors,
with single author entries preceding multiple author entries. Order
references by the same authors by the year of publication, with the
earliest first.

Use a first level section heading, ``{\bf References}'', as shown
below. Use a hanging indent style, with the first line of the
reference flush against the left margin and subsequent lines indented
by 1/8~inch. Below are example references for a conference paper, book
chapter, journal article, dissertation, book, technical report, and
edited volume, respectively.

\nocite{ChalnickBillman1988a}
\nocite{Feigenbaum1963a}
\nocite{Hill1983a}
\nocite{OhlssonLangley1985a}
% \nocite{Lewis1978a}
\nocite{Matlock2001}
\nocite{NewellSimon1972a}
\nocite{ShragerLangley1990a}


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{CogSci_Template}


\end{document}
